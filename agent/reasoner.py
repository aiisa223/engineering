"""Reasoning module that interfaces with a local Ollama model."""

from __future__ import annotations

import re
import shutil
import subprocess
from typing import Optional


class Reasoner:
    def __init__(self, model_name: str = "gemma3") -> None:
        self.model_name = model_name
        self._has_ollama = bool(shutil.which("ollama"))

    def think(self, prompt: str) -> str:
        """Return reasoning output generated by the local LLM."""
        if not self._has_ollama:
            # Fall back to a simple echo if ollama is not installed.
            return f"[LLM unavailable] {prompt}"

        result = subprocess.run(
            ["ollama", "run", self.model_name, prompt],
            capture_output=True,
            text=True,
        )
        output = result.stdout if result.returncode == 0 else result.stderr
        return output.strip()

    def extract_code(self, text: str) -> Optional[str]:
        """Extract the first Python code block from the text."""
        match = re.search(r"```python\n(.*?)```", text, re.DOTALL)
        if match:
            return match.group(1)
        return None
